{
  "master": {
    "tasks": [
      {
        "id": 1,
        "title": "Setup Docker Compose for n8n Environment",
        "description": "Create a `docker-compose.yml` file to launch the n8n container. This setup must include volume mounts for local data and script folders to allow n8n to access and execute Python modules on the host machine.",
        "details": "Based on requirements D.1.1 and D.1.2. The docker-compose file should map a local `./data` directory and a `./scripts` directory into the container. The n8n service should be exposed on port 5678. Ensure proper user permissions are considered for file access between the container and the host.",
        "testStrategy": "Run `docker-compose up`. Verify that the n8n container starts successfully and that files created in the mapped local directories are visible from within the n8n UI or container shell.",
        "priority": "high",
        "dependencies": [],
        "status": "done",
        "subtasks": []
      },
      {
        "id": 2,
        "title": "Implement Base `diarization_module` Script",
        "description": "Create the initial Python script for `diarization_module`. This script will take a video file path as input and use a library like `whisper-diarization` to perform speaker separation and generate a `source.srt` file.",
        "details": "The script should be executable from the command line. The output `source.srt` must strictly follow the format `[Speaker X]: [Dialogue Content]` as specified in P.1.4. Initial implementation can target CPU to validate the core logic first.",
        "testStrategy": "Execute the script with a sample .mp4 file. Verify that a `source.srt` file is generated and its content correctly reflects the speaker-dialogue format.",
        "priority": "high",
        "dependencies": [],
        "status": "in-progress",
        "subtasks": []
      },
      {
        "id": 3,
        "title": "Integrate ROCm for GPU Acceleration in `diarization_module`",
        "description": "Modify the `diarization_module` to leverage the AMD RX 9060 XT GPU using the ROCm stack. This involves configuring the underlying ML framework (e.g., PyTorch) to use the ROCm device instead of CUDA.",
        "details": "Addresses requirements P.1.1, P.1.2, and P.1.3. The local Python environment must have a ROCm-compatible version of PyTorch installed. The script needs to explicitly set the device to the AMD GPU. Compatibility with the RX 9060 XT must be confirmed.",
        "testStrategy": "Run the diarization script on a large video file. Monitor GPU utilization using a tool like `rocm-smi` to confirm the RX 9060 XT is actively being used during the process. Compare execution time with the CPU-only version.",
        "priority": "high",
        "dependencies": [
          2
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 4,
        "title": "Implement `llm_correction_module` with Gemini API",
        "description": "Develop the Python script for `llm_correction_module`. This script will parse the `source.srt` file, send only the dialogue text to the Gemini API for correction, and then reconstruct a `cleaned.srt` file, preserving all original timestamps and speaker tags.",
        "details": "Implements P.2.1 and P.2.2. The script must include robust SRT parsing/reconstruction logic. The prompt sent to Gemini must contain strong instructions to only correct grammar and phrasing, without altering the speaker labels or timecodes.",
        "testStrategy": "Process a sample `source.srt` file. Compare the input `source.srt` and output `cleaned.srt` files. Verify that timestamps and `[Speaker X]` tags are identical, and only the dialogue text has been modified.",
        "priority": "medium",
        "dependencies": [
          2
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 5,
        "title": "Implement `tts_module` for Audio Generation",
        "description": "Create the `tts_module` Python script. It will read the `cleaned.srt` file, and for each entry, use the Chatterbox library to generate an audio segment for the dialogue.",
        "details": "This script will handle parsing the SRT file and making individual calls to the Chatterbox TTS engine. Initially, it can use a single voice for all speakers to establish the generation pipeline.",
        "testStrategy": "Run the script with a `cleaned.srt` file. Verify that a set of individual audio files (e.g., .wav or .mp3) are created, one for each line of dialogue in the SRT.",
        "priority": "medium",
        "dependencies": [
          4
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 6,
        "title": "Enhance `tts_module` with Speaker-to-Voice Mapping",
        "description": "Update the `tts_module` to dynamically assign different Chatterbox voices to different speakers based on the `[Speaker X]` tags in the `cleaned.srt` file.",
        "details": "As per requirement P.3.1. The script should maintain a mapping (e.g., a dictionary) from speaker tags like `[Speaker 1]` to specific Chatterbox voice IDs. This ensures that each speaker has a unique and consistent voice throughout the audio.",
        "testStrategy": "Process an SRT file with at least two different speakers. Listen to the generated audio clips and confirm that the voice changes when the speaker tag changes.",
        "priority": "medium",
        "dependencies": [
          5
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 7,
        "title": "Implement Timeline-based Audio Merging in `tts_module`",
        "description": "Add functionality to the `tts_module` to combine all generated audio segments into a single, final audio file. The merging process must respect the timestamps from the `cleaned.srt` to ensure correct pacing and silence between spoken lines.",
        "details": "Fulfills requirement P.3.2. This will likely involve using an audio manipulation library like pydub or ffmpeg-python to sequence the audio clips and insert silence based on the start and end times in the SRT file.",
        "testStrategy": "Generate a final audio file from a multi-line SRT. Play the audio file alongside the original video to verify that the generated speech is correctly synchronized with the video's dialogue timings.",
        "priority": "medium",
        "dependencies": [
          6
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 8,
        "title": "Configure n8n Workflow and Secure Credentials",
        "description": "Build the workflow in the n8n UI. Set up the Gemini API key using n8n's built-in Credentials feature to avoid exposing it directly in the workflow. Configure 'Execute Command' nodes to call the local Python scripts.",
        "details": "Implements D.1.3 and D.1.4. Create a new credential for the Gemini API key. Create three 'Execute Command' nodes, one for each Python module. Configure the commands to correctly call the python interpreter and pass file paths as arguments.",
        "testStrategy": "Create a simple test workflow that passes the stored Gemini API key to a local script that just prints it. Verify the script receives the key correctly without it being visible in the n8n workflow's JSON.",
        "priority": "low",
        "dependencies": [
          1
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 9,
        "title": "Integrate Modules into a Cohesive n8n Pipeline",
        "description": "Connect the 'Execute Command' nodes in the n8n workflow to create the full P.1 -> P.2 -> P.3 pipeline. Ensure the output file from one step is correctly used as the input for the next step.",
        "details": "This task focuses on the data flow within n8n. The first node will execute `diarization_module.py`, the second will take its output (`source.srt`) and run `llm_correction_module.py`, and the third will take its output (`cleaned.srt`) and run `tts_module.py`.",
        "testStrategy": "Manually place an input video file in the data directory. Trigger the workflow. Check that `source.srt`, `cleaned.srt`, and the final audio file are generated in the correct sequence in the output directory.",
        "priority": "low",
        "dependencies": [
          3,
          4,
          7,
          8
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 10,
        "title": "End-to-End System Test and Validation",
        "description": "Perform a complete, automated test of the entire system. This involves triggering the n8n workflow with a new video file and verifying the final audio output without any manual intervention.",
        "details": "This is the final validation task (T.5). The test should cover the entire process: video input, GPU-accelerated diarization, LLM correction, and multi-speaker TTS generation. The final output should be a high-quality audio file with distinct voices synced to the original video's dialogue.",
        "testStrategy": "Execute the full n8n workflow with a new test video containing multiple speakers. The workflow must complete without errors. The final audio file must have clearly distinguishable voices for different speakers and maintain correct timing.",
        "priority": "medium",
        "dependencies": [
          9
        ],
        "status": "pending",
        "subtasks": []
      }
    ],
    "metadata": {
      "created": "2025-10-28T12:15:49.583Z",
      "updated": "2025-10-28T12:35:37.048Z",
      "description": "Tasks for master context"
    }
  }
}